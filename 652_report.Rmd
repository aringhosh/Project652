---
title: "STAT 652 Final Project"
author: "Arin Ghosh"
date: "12/1/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```


## 1. Introduction

This is an analysis report of the data provided by the Canadian Community Health Survey (CCHS) – Healthy Aging module. The project is divided into 2 parts corresponding to two separate datasets provided to us. The first dataset has 20000 rows and 9 columns. In this dataset problem our task is to predict cognitive health index called HUIDCOG using 8 other health-utility-index (HUI) variables. In the second part of the project, we are tasked with building a regression model that predicts a real number called HUIDHSI, which is another measure of the HUI that provides a description of an individual’s overall functional health. The second dataset is much bigger in size compared to the first dataset, having 590 variables with 10000 rows. A part of the dataset is held out for validation purpose which was released to the students at a later date.

## Part 1: Predicting HUIDCOG (Classification Analysis)

## 1 Data

### 1.1 Data Loading
First step is to load the appropriate dataset into the R Studio environment. The dataset can be found on the project github repository. Once downloaded in to the working directory of the R Studio, we can load the data using read.csv() command.
```{r}
hui <- read.csv("hui.csv")
summary(hui)
```

From the summary of the hui dataset, we can see that there are a total of 9 columns that exists. Further, we see from the summary that there are no NA or missing values, although there are several entries with value 'NOT STATED'. Also it is noteworthy to mention that none of the variables are continuous variables and the value which we are suppose to predict is a multivariate i.e. HUIDCOG can take one of the possible 7 values for each set of dependent variables. Although the dataset appear to be clean and variables are grouped, we will still need to do some analysis and take a look if we can reduce possible number of outcomes for each column without loosing a lot of quality in the dataset where it makes sense.

### 1.2 Exploratory Data Analysis & Data Grouping

#### Missing Data:
To start with our data analysis, we can notice that for some of the responses, we have an inconclusive response i.e. ```hui$HUIDCOG== 'NOT STATED'```. We should first remove these records, since this leads to an observation where we don't know what is the outcome. 
```{r}
# remove NOT STATED from HUIDCOG
hui <- hui[hui$HUIDCOG!= 'NOT STATED',]
hui$HUIDCOG <- factor(hui$HUIDCOG)
```

#### Target Variabe:
The given dataset is a fully categorical mulltivariate dataset meaning there are no columns with real numbers. This is something that is not extinsively discussed in our class, although we've been tought how to deal with categorical data in general. Since multivariate analysis is not extensively covered in the coursework, I am going to reduce the possible outcome of our dependent variable HUIDCOG from 6 to 2. To come up with a meaningful division, we must refer to the original documentation provided by the instructor. If we take a look into page 53 of CCHS_HA_Derived_variables.pdf then we can find out how these 6 different classes of HUIDCOG came into existence. For our analysis purpose I have divided our target variable HUIDCOG into a binary response where 1 refers to the patient is healthy in terms of cognitive abilities and 0 is unhealthy. We assign 1 if the patient is able to think clearly and solve day to day problems (COG. ATT. LEVE 1) and assign a 0 otherwise since in any other case it indicates some kind of issues with the patient's cognitive health abilities.
```{r}
hui$HUIDCOG = ifelse(hui$HUIDCOG=='COG. ATT. LEVE 1', 1, 0)
```

#### Dependent Variable Removal:

Next we are interested in seeing the different class distribution of the 8 dependent variables. Particularly the variables HUIGDEX and HUIGSPE.

```{r, echo=FALSE, fig.width=4, fig.height=3, message=FALSE}
library(dplyr)
dep <- dplyr::select(hui, -c(HUIDCOG))
lapply(dplyr::select(dep, c(HUIGDEX, HUIGSPE)),function(freq_dist) {prop.table(table(freq_dist))})
```
Both of these variables, in my opinion, lacks diversity and is biased heavily towards one class than the others, therefore I have decided not to include these 2 variables in my analysis.
```{r}
hui = dplyr::select(hui, -c(HUIGDEX, HUIGSPE))
```


#### Dependent Variable Group Collapsing:

Furthermore, I have tried to reduce the number of groups for each of the remaining 6 variables to some degree where it makes sense. For example, instead of using 4 possible classes of HUIGHER, which classifies the respondents based on their hearing state, I have collapsed it into 2, marking HUIGHER as Good if there was a history of hearing problem (whether or not its corrected currently) or otherwise Bad there was no hearing complains ever for that particular patient. Below is a summary of the collapsing decisions that has been made in this analysis.

##### HUIDEMO : Emotional index
This variable classifies respondents based on emotional health status. The original record has 6 different levels based on different levels of emotional response. But we can reduce it to Happy or Unhappy based on broader definition. I have converted all of NOT STATED as Unhappy.
```{r, echo=F}
lookup = data.frame(HUIDEMO=levels(hui$HUIDEMO), isHappy=c('Happy', 'Happy', 'Unhappy','Unhappy','Unhappy','Unhappy'))
hui = dplyr::select(merge(hui, lookup, by="HUIDEMO"), -HUIDEMO)


knitr::kable(
  lookup,
  caption = 'Mapping Table of HUIDEMO'
)  
```

##### HUIGHER : Hearing State
This variable classifies respondents based on hearing state of the patient. As explained earlier, the original 4 possible classes are reduced to a broader 2 general classes of Good or Bad indicating if the patient has a history of hearing issues.
```{r, echo=F}
lookup = data.frame(HUIGHER=levels(hui$HUIGHER), 
                    hearingState=c('Good', 'Bad', 'Good','Bad'))
hui = dplyr::select(merge(hui, lookup, by="HUIGHER"), -HUIGHER)

knitr::kable(
  lookup,
  caption = 'Mapping Table of HUIGHER'
)
```


##### HUIGMOB : Mobility Trouble
This variable classifies the respondents based on their state of mobility trouble. We classify this as TRUE or FALSE indicating if the respondent indicated that (s)he cannot move freely without external help.  
```{r, echo=F}
lookup = data.frame(HUIGMOB=levels(hui$HUIGMOB), 
                    mobilityHelp=c(T, F, F, F,T))
hui = dplyr::select(merge(hui, lookup, by="HUIGMOB"), -HUIGMOB)

knitr::kable(
  lookup,
  caption = 'Mapping Table of HUIGMOB'
)
```

##### HUIGVIS : Vision State
This variable classifies the respondents based on their vision state. Like HUIGMOB, I have mapped this to TRUE if the respondent has a history of vision problem and False otherwise.
```{r, echo=F}
lookup = data.frame(HUIGVIS=levels(hui$HUIGVIS), 
                    visualProb=c(T, T, F, F))
hui = dplyr::select(merge(hui, lookup, by="HUIGVIS"), -HUIGVIS)

knitr::kable(
  lookup,
  caption = 'Mapping Table of HUIGVIS'
)
```


##### DHHGAGE : Age
Instead of age groups, I have take the mean age of the group, although since I am not reducing the number of classes here, it is probably not going to add a lot of value in the complexity reduction of our final model, unless we decide not to use this variable. 
```{r, echo=F}
lookup = data.frame(DHHGAGE=levels(hui$DHHGAGE), meanAges=c(47, 52, 57, 62, 67, 72, 77, 82, 87))
hui = dplyr::select(merge(hui, lookup, by="DHHGAGE"), -DHHGAGE)

knitr::kable(
  lookup,
  caption = 'Mapping Table of DHHGAGE'
)
```

Here is the glance of the final dataset after data cleaning that we will be using in our model building.
```{r, echo=F}
knitr::kable(
  head(hui),
  caption = 'Final Dataset To Be Used For Model Building'
)

summary(hui)
```


## 2 Methods

In this part of the project, I have used some of the classification techniques that were taught over the course from Logistic Regression to Support Vector Machines. However for focussing on one technique, **Logistic Regression** is preferred which is interpretable and gives a low missclassification error rate as well as decent Specificity and Sensitivity score. I have used the variable importance table from random forest models to select a subgroup of variables to further tune my models. I have attached the details into the **Appendix** section.

### 2.1 Logistic Regression

Logistic Regression uses the logistic function fitted by **maximum likelihood**. It performs well even if the predictors do not follow Gaussian distribution. The model is a linear model in the log-odds of success
$$ \log \left( \frac{p(X)}{1-p(X)} \right) = 
X \beta = \beta_0 + X_1 \beta_1 + \ldots + 
X_p \beta_p. $$

Since our dependent variable takes a 0/1 binary response, we can use this model. Unlike linear regression where one unit change in the predictor variable (X) results in one unit change in Y, here one unit increase in $X_j$ , while holding all others fixed is associated with a $\beta_j$ change in the log-odds.

Let’s start with baseline model in logistic regression. Before fitting the model, dataset is split into training and testing set in random sampled fashion of ratio 70:30. The model is fitted to train data and then predict the test data to validate based on its accuracy, sensitivity, specificity, etc.

The coefficients must be estimated based on the available training data. For logistic regression, the more
general method of maximum likelihood is preferred for its robust statistical properties. Basically, the algorithm tries to find coefficients that maximize the likelihood that the probabilities are closest to 1 for people who don't have any problem in terms of patient's cognitive abilities (i.e. the respondent is able to think clearly and can solve day to day problems), and close to zero for people who has some type of cognitive disability and cannot carry out their day-to-day activity without some degree of help. 
During my experiment I have found that isHappy, mobilityHelp, hearingState, and meanAges are the most important variables so I have included only these 4 variables in my final model. Below table shows summary of estimates, Std.Error and p-value in the order of significance after performing logistic regression to the training data.


```{r, echo=F}
glm.model <- glm(HUIDCOG ~ isHappy + mobilityHelp + hearingState + meanAges,
               data=hui, family = binomial())
coef_table <- summary(glm.model)$coefficients
coef_table[,"Pr(>|z|)"] = format(coef_table[,"Pr(>|z|)"], digits = 3)

knitr::kable(
  coef_table,
  caption = 'Summary of Final Logistic Regression and its odd-ratios'
)
```

And here is the confusion matrix for the model indicating various measurements including accuracy, its 95% confidence interval, sensitivity, specificity and balanced accuracy as well.

```{r, echo=F, message=FALSE, warning=FALSE}
library(caret)
pred <- predict(glm.model, hui)
pred_class <- ifelse(pred > 0.5, 1, 0)
confusionMatrix(hui$HUIDCOG, pred_class)
```

## 3. Results

### 3.1 Model Interpretation

On examining the fitted logistic regression model summary above, we can see that all the predictors are **statistically significant** with p-values far less than required 5%. These results also concur with the findings that predictors such as HUIGDEX, HUIGSPE, and DHH_SEX are not very significant predictors in determining a respondents cognitive health state. When we compare the full model of all available variables *(DHH_SEX + meanAges + isHappy + hearingState + mobilityHelp + visualProb)* with that of a model build on the subset of 4 predictors $(isHappy + mobilityHelp + hearingState + meanAges)$  using Anova chisq test, a small p-value $(7.693e-08) < 0.05$ indicated that both models are similar - thus parsimoniously we chose the smaller model. Also, we have checked with various available logistic  regression model selection techniques such as best subset, forward, backward and step-wise selection, all of them pointed to the four predictors that was used in the final model build. Therefore the final model that we've selected for predicting **HUIDCOG** is: $$ y_i = \beta_0 + isHappy * \beta_1 + mobilityHelp * \beta_2 + hearingState * \beta_3 + meanAges * \beta_4 $$
We can interprete the model in this way: If a respondent reported that (s)he requires mobility help == 1, keeping other predictor unchanged, that can be associated with an estimated increase of (-0.53) units in the log-odds of the respondent being cognitively healthy. We can see from the Table 7 that it is estimated that if the respondent is classified as happy or need no mobility help or has a good hearing state or is younger (selecting any one of these predictor while not changing the others) it generally is an indication of the respondent is cognitively healthy, which makes sense. Now we can not only point out which predictor is associated with diminishing cognitive ability but we can also indicate more relevant statistics which is by how much units they affect the mental state of the patients.

#### 3.2 Model Evaluation

Logistic regression model is used to predict the test data as well as the validation dataset that was released later to the students. We have used various statistical measures to measure the effectiveness of this model such as missclassification rate, sensitivity, specificity and Area under the ROC curve. We have also tried k fold cross validation on the training data set. Below is a graph that shows the ROC curve plot that is derived on the test dataset. The value of Area under the ROC curve we got is ~0.61. Based on various cut-off values, we found a cut-off of 0.5 leads to the best balance of accuracy, sensitivity and specificity. Please refer to the Appendix for further details on this section's derivations. 

#### 3.3 Comparisons of Classification Models

This sections shows the various models that we tried along with Logistic Regression model that was ultimately selected. The classification was model was build on 70% of the available data and 30% of the data was used for testing. Different metric that are used to compare the models are discussed in the following:

**Misclassification Error:** The number of observations that were predicted wrongly by the model. It is the
proportion of misclassified observations.

**Sensitivity:** It is the ability of a model to correctly identify those with diabetic disease. It is observed True positive rate. TP/(TP+FN) where TP is True Positive and FN is False Negative.

**Specificity:** It is the ability of a model to correctly identify those without diabetic disease. It is observed True negative rate. TN/(TN+FP) where TP is True Negative and FP is False Positive.


```{r, echo=FALSE}
# TODO different models
method = c('Logistic Regression', 'LDA', 'QDA', "Random Forest")
accuracy = c(0.7003333, 0.7088333, 0.6938333, 0.707)
sensitivity = c(0.4978903, 0.5521064, 0.4805781, 0.5461538)
specificity = c(0.7383215, 0.7215715, 0.7420805, 0.7181818) 
lookup = data.frame(method, accuracy, specificity, sensitivity, row.names = NULL)
knitr::kable(
  lookup,
  caption = 'Model Comparison - Predicting HUIDCOG'
)
```


#### Evaluation of best model on the Holdout dataset

Once I finalized the model, I have re-fit the model on the full dataset (without any tran-test split) and then used the holdout dataset to measure the accuracy, sensitivity & specificity which I got is :

  * accuracy : 0. 70  
  * sensitivity : 0.502  
  * specificity : 0.729  


## Conclusion and Discussion

After going through all the models, we have picked the logistic regression model with four predictors as our final selected model. However we have only considered linear terms in this model, which can be a drawback in this model. Therefore in the future it will be noteworthy to check how adding interaction and non-linear terms changes the predictive ability of the logistic regression model, or if there are any other family of classification techniques stands out when these new terms are introduced. We can try for even more powerful models such as deep neural networks to see if that helps us to give a better result. Finally all these discussions are based on the dataset that was provided to us. The correctness of this model will change as new data are available to us in the future, so that we might have to tune it later.

\newpage

## Appendix 1 (For Part 1)

This section mainly deals with the code base that was used for the analysis part written for part 1.

### Software Version:

  * All analysis on this project was done using R Studio - version 1.1.456 – © 2009-2018 RStudio, Inc.
  * OS - Mac OS v 10.14.1 (18B75)

### Data Loading:
Load all the required packages and some helper functions
```{r library_chunk, echo = T, results = 'hide', warning=FALSE, message=F}
library(dplyr, quietly = T)
library(caret, quietly = T)
library(MASS, quietly = T)
library(pROC, quietly = T)
library(randomForest, quietly = T)
library(leaps, quietly = T)
library(caret, quietly = T)
library(gam, quietly = T)

getCMMeasurements <- function(cm){
  accuracy = cm$overall[[1]]
  sensitivity = cm$byClass[[1]]
  specificity = cm$byClass[[2]]
  result <- t(as.data.frame(c(accuracy, sensitivity, specificity)))
  colnames(result) <- c('accuracy', 'sensitivity', 'specificity')
  rownames(result) <- NULL
  return(result)
}
```

Read the csv file and get a summary of the data
```{r}
hui <- read.csv("hui.csv")
summary(hui)
```

We are going to filter our target classes into 2 classes, based on the COG. ATT. LEVE value for HUIDCOG column. We say 1 if the respondent is doing alright, and 0 if there's some risk of imperfect cognitive attention level.
```{r , fig.height = 3, fig.width = 3, fig.align = "center", fig.cap="\\label{fig:figs}distribution of target variable"}
hui$HUIDCOG = as.factor(ifelse(hui$HUIDCOG=='COG. ATT. LEVE 1', 1, 0))
plot(hui$HUIDCOG)
```

### Data Cleaning

We first need to remove all the incomplete responses.
```{r}
# remove NOT STATED
hui <- hui[hui$HUIDCOG!= 'NOT STATED',]
hui$HUIDCOG <- factor(hui$HUIDCOG)
dim(hui)
```

Next we are going to go through each of the available independent variable columns to see if we can further group them down.
The details of every variable:

+ DHHGAGE - This variable indicates the age of the selected respondent. (p29)  
+ DHH_SEX - sex of the patient  
+ <Target> HUIDCOG - Cognition (Function Code) This variable classifies respondents based on cognitive health status. (p53)  
+ HUIGDEX - This variable classifies the respondents based on their state of dexterity trouble. (p55)  
+ HUIDEMO - This variable classifies respondents based on emotional health status. (p55)  
+ HUIGHER - This variable classifies the respondents based on their hearing state. (p57)  
+ HUIGMOB - This variable classifies the respondents based on their state of mobility trouble. (p59)  
+ HUIGSPE - This variable classifies the respondents based on their state of speech trouble. (p60)  
+ HUIGVIS - This variable classifies the respondents based on their vision state. (p62) 

Age: We are replacing each with mean of the age class
```{r}
lookup = data.frame(DHHGAGE=levels(hui$DHHGAGE), 
                    meanAges=c(47, 52, 57, 62, 67, 72, 77, 82, 87))
hui = dplyr::select(merge(hui, lookup, by="DHHGAGE"), -DHHGAGE)

rm(lookup)
```

Emotional index, happy/not happy
```{r}
lookup = data.frame(HUIDEMO=levels(hui$HUIDEMO), 
                    isHappy=c('Happy', 'Happy', 'Unhappy','Unhappy','Unhappy','Unhappy'))
hui = dplyr::select(merge(hui, lookup, by="HUIDEMO"), -HUIDEMO)
```

Hearing State
```{r}
lookup = data.frame(HUIGHER=levels(hui$HUIGHER), 
                    hearingState=c('Good', 'Bad', 'Good','Bad'))
hui = dplyr::select(merge(hui, lookup, by="HUIGHER"), -HUIGHER)
```

Mobility Trouble
```{r}
lookup = data.frame(HUIGMOB=levels(hui$HUIGMOB), 
                    mobilityHelp=c(T, F, F, F,T))
hui = dplyr::select(merge(hui, lookup, by="HUIGMOB"), -HUIGMOB)
```

Vision State
```{r}
lookup = data.frame(HUIGVIS=levels(hui$HUIGVIS), 
                    visualProb=c(T, T, F, F))
hui = dplyr::select(merge(hui, lookup, by="HUIGVIS"), -HUIGVIS)
rm(lookup)
```

We can also see that both HUIGDEX and HUIGSPE are highly biased towards one particular class
```{r}
table(hui$HUIGDEX)
table(hui$HUIGSPE)
```

Therefore we remove these columns with high bias from our dataset:
```{r}
hui = dplyr::select(hui, -c(HUIGDEX, HUIGSPE))
```

The final dataset looks like this:
```{r}
summary(hui)
```

### Train Test Split 

The dataset is split into 70/30 ratio
```{r}
## 70% of the sample size
smp_size <- floor(0.70 * nrow(hui))

## set the seed to make your partition reproducible
set.seed(19)
train_ind <- sample(seq_len(nrow(hui)), size = smp_size)

hui.train <- hui[train_ind, ]
hui.test <- hui[-train_ind, ]

rm(smp_size)
```

### Model Fitting & Analysis

##### Logistic Regression without any subset selection on the test dataset
```{r}
full.model.glm <- glm(HUIDCOG ~ ., data=hui.train, family = binomial())
summary(full.model.glm)
```
Other than DHH_SEXMALE, every other predictor looks significant.

Subset Selection using forward/backward and stepwise
```{r}
cfits.fwd <- regsubsets(HUIDCOG ~ .,data=hui.train, method="forward")
plot(cfits.fwd,scale="Cp")
rm(cfits.fwd)
```

Forward subset selection method concurs the finding in the Logistic Regression summary above, DHH_SEXMALE is still something we can get rid of. Also We get the same kind of result using the backward and seqrep method as well, so its not included.

Based on these analysis, we are going with the following reduced subset of independent variables - isHappy, mobilityHelp, hearingState, meanAges & visualProb.
```{r}
final.model.FUN <- (HUIDCOG ~ isHappy + mobilityHelp + hearingState + meanAges + visualProb)
```

#### We can now build our first logistic regression model
```{r}
small.model.glm <- glm(final.model.FUN,data=hui.train, family = binomial())
```

Is this model any better than the full model that we had earlier?
```{r}
# Anova Test will tell us
anova(full.model.glm, small.model.glm, test ="Chisq")
```

We can accept the null hypothesis that these 2 models are the basically the same. But going by the parsimony, we will choose the lighter model.

So we can now do some prediction and see how good our model is
```{r, fig.height=3, fig.width= 3}
pred <- predict(small.model.glm, hui.test)
pred_class <- ifelse(pred > 0.5, 1, 0)
roc.curve <- roc(hui.test$HUIDCOG, pred, direction="<")
print(roc.curve)
plot(roc.curve,col="blue", lwd=5)
```

```{r, echo=F}
#consfusion matrix
cm <- caret::confusionMatrix(hui.test$HUIDCOG, pred_class)
cm

knitr::kable(
  getCMMeasurements(cm),
  caption = 'Logistic Regression Metrics'
)
```

Based on this we can see that our accuracy for this model is 70.03% while the calculated sensitivity is 49% & the specificity is 73%. Our Area under the ROC curve is 62.26%

Does KFold Cross Validation makes our prediction better? I have written a small function following the lecture notes to implement it. Here we do a 10 fold cross validation.
```{r}
set.seed(19)
k_fold <- 10
cv.err <- rep(NA,k_fold)
cv.sen <- rep(NA,k_fold)
cv.spec <- rep(NA,k_fold)

#Randomly shuffle the data
hui<-hui[sample(nrow(hui)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(hui)),breaks=10, labels=FALSE)
#Perform 10 fold cross validation
for(i in 1:k_fold){
  #Segement your data by fold using the which() function 
  testIndexes <- which(folds==i, arr.ind=TRUE)
  testData <- hui[testIndexes, ]
  trainData <- hui[-testIndexes, ]
  #Use the test and train data partitions however you desire...
  model <- glm(final.model.FUN, data=trainData, family = binomial())
  pred <- predict(model, testData)
  pred_class <- ifelse(pred > 0.5, 1, 0)
  cm <- confusionMatrix(testData$HUIDCOG, pred_class)
  cv.err[i] <- cm$overall[[1]]
  cv.sen[i] = cm$byClass[[1]]
  cv.spec[i] = cm$byClass[[2]]
}

# print(mean(cv.err))
result <- data.frame(mean(cv.err), mean(cv.sen), mean(cv.spec))
colnames(result) <- c('accuracy','sensitivity', 'specificity')
knitr::kable(
  result,
  caption = 'K-Fold Cross Validation Result for Logistic Regression'
)
```

Due to computational complexity, I have not performed K-Fold cross validation for all of the models discussed here.

#### Linear Discriminant Analysis:
```{r}
lda.model <- lda(final.model.FUN, data=hui.train)
lda.pred <- predict(lda.model, hui.test, type= 'class')
pred_class <- lda.pred$class
plot(lda.model)
```

```{r, echo=F}
#consfusion matrix
cm <- caret::confusionMatrix(hui.test$HUIDCOG, pred_class)

knitr::kable(
  getCMMeasurements(cm),
  caption = 'LDA Metrics'
)
```

We get a slightly better accuracy and sensitivity but the specificity dropped compared to logistic regression.

#### Quadratic Discriminant Analysis
```{r}
qda.model <- qda(final.model.FUN, data=hui.train)
qda.pred <- predict(qda.model, hui.test, type= 'class')
pred_class <- qda.pred$class
```

```{r, echo=F}
#consfusion matrix
cm <- caret::confusionMatrix(hui.test$HUIDCOG, pred_class)

knitr::kable(
  getCMMeasurements(cm),
  caption = 'QDA Metrics'
)
```

The QDA apparently don't promise any improvement over other models. 

#### Random Forest:
```{r}
set.seed(19)
tr <- randomForest::randomForest(final.model.FUN, data=hui.train, 
                   ntree = 100, mtry= 5)
pred_class <- predict(tr, hui.test, type= 'class')
```

```{r, echo=F}
#consfusion matrix
cm <- caret::confusionMatrix(hui.test$HUIDCOG, pred_class)

knitr::kable(
  getCMMeasurements(cm),
  caption = 'Random Forest Metrics'
)
```

So far Random Forest have the best accuracy and sensitivity.

#### General Additive Model

Generalized additive models (GAMs) extend a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity. In this problem, upon further analysis, we extended non-linearity to one of the predictor meanAges by adding spline function to it since its non-linear form appears statistically significant.
```{r}
gam.fit <- gam(HUIDCOG ~ isHappy + mobilityHelp + hearingState + s(meanAges,2),
           data=hui.train,family=binomial)
summary(gam.fit)
```
